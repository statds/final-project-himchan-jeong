---
title: "Predictive models for P&C insurance"
author:
- Himchan Jeong
institute: University of Connecticut
output:
  beamer_presentation:
    colortheme: orchid
    slide_level: 1
    template: default.tex
    theme: madrid
  slidy_presentation: default
shorttitle: "STAT 6494: Data Science in Action"
date: "6 March, 2018"
place: "Data Science in Action"
header-includes:
- \usepackage{bbm}
- \usepackage{hhline}
- \newcommand{\E}[1]{{\mathbb E}\left[#1\right]}
- \newcommand{\Var}[1]{{\mathrm Var}\left(#1\right)}
- \usepackage{beamerthemesplit}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{caption}
- \newcommand\tab[1][1cm]{\hspace*{#1}}
- \captionsetup[table]{belowskip=0pt,aboveskip=0pt}
- \captionsetup[figure]{belowskip=0pt,aboveskip=0pt}
---

# What is Actuarial Science? 
\begin{itemize}
\item "Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in insurance, finance and other industries and professions." (Wikipedia)
\item In short, We need to PRICE given risk for the transaction.
\item Thus, actuaries need well-developed predictive model both with high predictability and interpretability.
\end{itemize}

# Interpretability Issue on Actuarial Science
There are a lot of reasons why the interpretability is important in Actuarial Science.
\begin{itemize}
\item Tradition
\item Internal/External Communication
\item Regulation
\item Robustness
\end{itemize}

# Purpose of the Project
\begin{itemize}
\item Introduce current practice done by property and casualty (P\&C) insurance company
\item Suggest the more sophisticated predictive model which can outperform the benchmarks
\end{itemize}

# P\&C Insurance Claim Data Structure 
\begin{itemize}
\item For ratemaking in P\&C, we have to predict the cost of claims $S = \sum\limits_{k=1}^N C_k$.
\item Policyholder $i$ is followed over time $t=1,\ldots,T_i$ years.
\item Unit of analysis ``$it$'' -- an insured driver $i$ over time $t$ (year)
\item For each ``$it$'', could have several claims, $k=0,1,\ldots,N_{it}$
\item Have available information on: number of claims $n_{it}$, amount of claim $c_{itk}$, exposure $e_{it}$ and covariates (explanatory variables)  $x_{it}$
\smallskip
\begin{itemize}
\item covariates often include age, gender, vehicle type, building type, building location, driving history and so forth
\end{itemize}
\end{itemize}

# Current Approches for Claim Modeling
\begin{itemize}
\item (1) Two-parts model for frequency and severity
\item (2) Tweedie model
\end{itemize}

# Two-parts Model
\begin{itemize}
\item Total claim is represented as following;
\begin{equation*}
\text{Total Cost of Claims }=\text{ Frequency }\times \text{Average Severity}
\end{equation*}
\item The joint density of the number of claims and the average claim size can be decomposed as
\begin{eqnarray*}
f(N,\overline{C}| \textbf{x}) &=& f(N| \mathbf{x}) \times f(\overline{C}|N, \textbf{x}) \\
\text{joint} &=& \text{frequency} ~\times~ \text{conditional severity}.
\end{eqnarray*}
\item In general, it is assumed $N \sim \text{Pois}(e^{X\alpha})$, and $C_i \sim \text{Gamma}(\frac{1}{\phi}, e^{X\beta}\phi)$.
\end{itemize}


# Tweedie Model
\begin{itemize}
\item Instead of dividing the total cost into two parts, Tweedie model directly entertain the distribution of compound loss $S$ where
$$
\begin{aligned}
&S = \sum_{k=1}^N C_k, \quad N \sim \text{Pois}(e^{X\alpha}) \\
&C_k \sim \text{Gamma}(\frac{1}{\phi}, e^{X\beta}\phi), \quad C_k \perp N \ \ \forall k
\end{aligned}
$$
\item It has point mass probability on $\{S=0\}$ and has the following property.
$$
\E{S} = \mu, \quad \Var{S} = \Phi \mu^{p}, \quad p \in (1,2)
$$
\end{itemize}

# Pitfalls in Current Practices
\begin{itemize}
\item (1) Dependence between the frequency and the severity
\smallskip
\item (2) Longitudinal property of data structure. 
  \begin{itemize}
  \item For example, if we observed a policyholder $i$ for $T_i$ years, then we have following observation $N_{i1},N_{i2},\ldots,N_{iT_i}$, which may not be identically and independently distributed.
  \end{itemize}
\end{itemize}

# Premium for Compound Loss under Independence
\begin{itemize}
\item If we assume that $N$ and $C_1,C_2,\ldots,C_n$ are independent, then we can calculate the premium for compound loss as
$$
\begin{aligned}
\E{S} &= \E{\sum_{k=1}^N C_k} = \E{\E{\sum_{k=1}^N C_k | N}} \\
      &= \E{\E{C_1+ \cdots + C_N | N}} = \E{N\E{C_1 | N}} \\
      &= \E{N \E{C}} = \E{N}\E{C}
\end{aligned}
$$
In other words, we just multiply the expected values from frequency model and the average severity model.
\item In general, $\E{S} \neq \E{N}\E{C}$.
\end{itemize}

# Why is the Dependence Important?
\begin{itemize}
\item If we have positive correlation between $N$ and $C$, then
$$
\E{S} > \E{N} \E{C}
$$
so the company suffers from the higher loss relative to earned premium.

\item If we have negative correlation between $N$ and $C$, then
$$
\E{S} < \E{N} \E{C}
$$
so the company confronts the loss of market share due to higher premium.
\end{itemize}

# Possible Alternatives for the Benchmarks
\begin{itemize}
  \item For dependence between the frequency and severity
    \begin{itemize}
    \item Set $\E{\overline{C}|N}=e^{X\beta+N\theta}$
    \item Copula for $N$ and $\overline{C}$
    \end{itemize}
\smallskip
  \item For longitudinal property
    \begin{itemize}
    \item Random effects model
    \item Copula for multiple claim observation
    \end{itemize}
\smallskip
  \item Non-traditional approaches
    \begin{itemize}
    \item Neural networks
    \item Regression for each group classified by decision tree
    \end{itemize}
\end{itemize}

```{r, message=FALSE, echo=FALSE}
setwd("C:/Users/HimChan/Desktop/DAction")
train1 <- read.csv("train.csv")
train <- train1[,c(2:3,8,10:15,17:21)]
rm(train1)
test1 <- read.csv("test.csv")
test <- test1[,c(2:3,8,10:15,17:21)]
rm(test1)
trainp <- subset(train,log(yAvgBC)>0)
```

# Data Description
\begin{itemize}
  \item Here I use a public dataset on insurance claim, provided by Wisconsin Propery Fund. \newline
        (https://sites.google.com/a/wisc.edu/jed-frees/)
  \item It consists of `r prettyNum(length(train$PolicyNum),big.mark=",")` observation in traning set and `r prettyNum(length(test$PolicyNum),big.mark=",")` observation in test set.
  \item It is a longitudinal data with more or less `r prettyNum(length(unique(train$PolicyNum)),big.mark=",")` policyholder, followed for `r max(train$Year) - min(train$Year)+1` years.
  \item Although the dataset includes information one multi-line insurance, here I only used building and contents (BC) claim information.
\end{itemize}

# Observable Policy Characteristics used as Covariates
\begin{center}
\resizebox{!}{2.75cm}{
\begin{tabular}{l|lrrr}
\hline \hline
Categorical & Description &  & \multicolumn{2}{c}{Proportions} \\
variables \\
\hline
TypeCity & Indicator for city entity:           & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeCity)/length(train$PolicyNum),2)` \%} \\
TypeCounty & Indicator for county entity:       & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeCounty)/length(train$PolicyNum),2)` \%} \\
TypeMisc & Indicator for miscellaneous entity:  & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeMisc)/length(train$PolicyNum),2)` \%} \\
TypeSchool & Indicator for school entity:       & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeSchool)/length(train$PolicyNum),2)` \%} \\
TypeTown & Indicator for town entity:           & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeTown)/length(train$PolicyNum),2)` \%} \\
TypeVillage & Indicator for village entity:     & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeVillage)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditBC & No BC claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditBC)/length(train$PolicyNum),2)` \%} \\
\hline
 Continuous & & Minimum & Mean & Maximum \\
 variables \\
\hline
CoverageBC  & Log coverage amount of BC claim in mm  &  `r round(min(train$CoverageBC),2)` & `r round(mean(train$CoverageBC),2)`
            & `r round(max(train$CoverageBC),2)`\\
lnDeductBC  & Log deductible amount for BC claim     &  `r round(min(train$lnDeductBC),2)` & `r round(mean(train$lnDeductBC),2)`
            & `r round(max(train$lnDeductBC),2)`\\
\hline 
FreqBC  & number of BC claim in a year  & `r round(min(train$FreqBC),2)` &
`r round(mean(train$FreqBC),2)`         & `r round(max(train$FreqBC),2)` \\
log(yAvgBC)  & (log) avg size of claim in a year  & `r round(min(log(trainp$yAvgBC)),2)` &
`r round(mean(log(trainp$yAvgBC)),2)`             & `r round(max(log(trainp$yAvgBC)),2)` \\
\hline \hline
\end{tabular}}
\end{center}


# Future Works for this Project
\begin{itemize}
  \item Deal with `outliers' on the observations for claim frequency.
  \item Provide methodologies for modelling the claim and compare their performance with those of the benchmark models.
  \item If possible, suggest a model with higher predictability and interpretability which can be used in P\&C insurance company.
\end{itemize}
