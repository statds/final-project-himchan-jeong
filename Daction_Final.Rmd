---
title: "Predictive modeling in P&C Insurance"
author:
- Himchan Jeong, University of Connecticut
output:
  pdf_document:
   number_sections: yes
   citation_package: natbib
bibliography: ./bibfile2018/daction2018.bib
geometry: margin = 0.7in
biblio-style: "apalike"
classoption: fleqn
header-includes:
- \usepackage{bbm}
- \usepackage{multirow}
- \usepackage{hhline}
- \usepackage{amsmath,amssymb}
- \usepackage{booktabs}
- \usepackage{caption}
- \captionsetup[table]{belowskip=4pt,aboveskip=0pt}
- \captionsetup[figure]{belowskip=0pt,aboveskip=4pt}
- \newcommand{\E}[1]{{\mathbb E}\left[#1\right]}
- \newcommand{\Var}[1]{{\mathrm Var}\left(#1\right)}
- \DeclareMathOperator*{\argmin}{argmin}
- \usepackage{float}
- \floatplacement{figure}{H}
---

\section{Introduction}

\subsection{What is Actuarial Science?}

According to wikipedia, Actuarial science is "the discipline that applies mathematical and statistical methods to assess risk in insurance, finance and other industries and professions. In short, we need to price given risk for the transaction. Actuary is one of the professions with 'data-driven decision making', for more than 200 years. So actuary can be classified as a type of data scientist whose expertise is in insurance and related industires. Thus, actuaries needs well-developed predictive model both with high predictability and interpretability.

There are a lot of reasons why the interpretability is important in Actuarial Science.
\begin{itemize}
\item Tradition: Since it has its own traditional education curriculum, which is provided by the Society of Actuaries (SOA) or Casualty Actuarial Society (CAS), the companies have their own developed pricing methods.
\item Internal/External Communication: Both senior managers and regulators are reluctant to use of unproven new method which might look like a `black-box'.
\item Robustness
\end{itemize}

I want to introduce current practice done by property and casualty (P\&C) insurance company, as well as suggest the more sophisticated predictive model which can outperform the benchmarks.

\subsection{Common Data Structure}

For ratemaking in P\&C, we have to predict the cost of claims $S = \sum\limits_{k=1}^n C_k$. Policyholder $i$ is followed over time $t=1,\ldots,T_i$ years. Unit of analysis $it$ -- an insured driver $i$ over time $t$ (year) For each $it$, we could have several claims, $k=0,1,\ldots,n_{it}$
Thus, we have available information on: number of claims $n_{it}$, amount of claim $c_{itk}$, exposure $e_{it}$ and covariates (explanatory variables)  $x_{it}$, which often include age, gender, vehicle type, building type, building location, driving history and so forth.

\subsection{Current Approches for Claim Modeling}

There are two major models which are well-known and widely used in P\&C insurance company. First one is two-parts model for frequency and severity, and the other is Tweedie model.

In two-parts model, total claim is represented as following;

$\qquad \text{Total Cost of Claims }=\text{ Frequency }\times \text{Average Severity}$

Therefore, the joint density of the number of claims and the average claim size can be decomposed as
\begin{eqnarray*}
f(N,\overline{C}| \textbf{x}) &=& f(N| \mathbf{x}) \times f(\overline{C}|N, \textbf{x}) \\
\text{joint} &=& \text{frequency} ~\times~ \text{conditional severity}.
\end{eqnarray*}
In general, it is assumed $N \sim \text{Pois}(e^{X\alpha})$, and $C_i \sim \text{Gamma}(\frac{1}{\phi}, e^{X\beta}\phi)$.

In tweedie Model, instead of dividing the total cost into two parts, we directly entertain the distribution of compound loss $S$ where
$$
\begin{aligned}
&S = \sum_{k=1}^N C_k, \quad N \sim \text{Pois}(e^{X\alpha}) \\
&C_k \sim \text{Gamma}(\frac{1}{\phi}, e^{X\beta}\phi), \quad C_k \perp N \ \ \forall k
\end{aligned}
$$
in order that it has point mass probability on $\{S=0\}$ and has the following property.
$$
\E{S} = \mu, \quad \Var{S} = \Phi \mu^{p}, \quad p \in (1,2)
$$
However, there are some pitfalls in the current practice aforementioned.
\begin{itemize}
\item (1) Dependence between the frequency and the severity
\smallskip
\item (2) Longitudinal property of data structure. 
  \begin{itemize}
  \item For example, if we observed a policyholder $i$ for $T_i$ years, then we have following observation $N_{i1},N_{i2},\ldots,N_{iT_i}$, which may not be identically and independently distributed.
  \end{itemize}
\end{itemize}

For the first problem, if we assume that $N$ and $C_1,C_2,\ldots,C_n$ are independent, then we can calculate the premium for compound loss as
$$
\begin{aligned}
\E{S} &= \E{\sum_{k=1}^N C_k} = \E{\E{\sum_{k=1}^N C_k | N}} \\
      &= \E{\E{C_1+ \cdots + C_N | N}} = \E{N\E{C_1 | N}} \\
      &= \E{N \E{C}} = \E{N}\E{C}
\end{aligned}
$$
In other words, we can just multiply the expected values from frequency model and the average severity model to get the estimate for compound loss. However, in general $N$ and $C_k$ are correlated so that $\E{S} \neq \E{N}\E{C}$. If we have positive correlation between $N$ and $C$, then
$$
\E{S} > \E{N} \E{C}
$$
so the company suffers from the higher loss relative to earned premium.

On the other hand, if we have negative correlation between $N$ and $C$, then
$$
\E{S} < \E{N} \E{C}
$$
so the company confronts the loss of market share due to higher premium.

There are some possible alternatives which can be used for dealing with the pitfalls. For example, @shi2015depfreqsev and @garrido2016glmdep used the observed frequency as a covariate for the average severity as following; $\E{\overline{C}|N}=e^{X\beta+N\theta}$.
    
In case of longitudinal property, @boucher2008depfreq analyzed various method and concluded that random effects model would be a good way for capturing the longitudinal property in P&C insurance.

Finally, we may suggest to use non-traditional method, such as regression with neural network or calculation of credibility premium per each group classified by decision tree.

```{r, message=FALSE, echo=FALSE}
setwd("C:/Users/HimChan/Desktop/DAction")
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

\section{Analysis}
\subsection{Data Description}
 Here I use a public dataset on insurance claim, provided by Wisconsin Propery Fund. (https://sites.google.com/a/wisc.edu/jed-frees/) It consists of `r prettyNum(length(train$PolicyNum),big.mark=",")` observation in traning set and `r prettyNum(length(test$PolicyNum),big.mark=",")` observation in test set. It is a longitudinal data with more or less `r prettyNum(length(unique(train$PolicyNum)),big.mark=",")` policyholder, followed for `r max(train$Year) - min(train$Year)+1` years. Since the dataset includes information on multi-line insurance, here I used building and contents (BC), inland marine (IM), and new motor vehicle (PN) claim information.

\newpage
\begin{table}[h!t!]
\begin{center}
\caption{Observable policy characteristics used as covariates} \label{tab:1}
\resizebox{!}{4cm}{
\begin{tabular}{l|lrrr}
\hline \hline
Categorical & Description &  & \multicolumn{2}{c}{Proportions} \\
variables \\
\hline
TypeCity & Indicator for city entity:           & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeCity)/length(train$PolicyNum),2)` \%} \\
TypeCounty & Indicator for county entity:       & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeCounty)/length(train$PolicyNum),2)` \%} \\
TypeMisc & Indicator for miscellaneous entity:  & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeMisc)/length(train$PolicyNum),2)` \%} \\
TypeSchool & Indicator for school entity:       & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeSchool)/length(train$PolicyNum),2)` \%} \\
TypeTown & Indicator for town entity:           & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeTown)/length(train$PolicyNum),2)` \%} \\
TypeVillage & Indicator for village entity:     & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeVillage)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditBC & No BC claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditBC)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditIM & No IM claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditIM)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditPN & No PN claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditPN)/length(train$PolicyNum),2)` \%} \\

\hline
 Continuous & & Minimum & Mean & Maximum \\
 variables \\
\hline
CoverageBC  & Log coverage amount of BC claim in mm  &  `r round(min(train$CoverageBC),2)` & `r round(mean(train$CoverageBC),2)`
            & `r round(max(train$CoverageBC),2)`\\
lnDeductBC  & Log deductible amount for BC claim     &  `r round(min(train$lnDeductBC),2)` & `r round(mean(train$lnDeductBC),2)`
            & `r round(max(train$lnDeductBC),2)`\\
CoverageIM  & Log coverage amount of IM claim in mm  &  `r round(min(train$CoverageIM),2)` & `r round(mean(train$CoverageIM),2)`
            & `r round(max(train$CoverageIM),2)`\\
lnDeductIM  & Log deductible amount for IM claim     &  `r round(min(train$lnDeductIM),2)` & `r round(mean(train$lnDeductIM),2)`
            & `r round(max(train$lnDeductIM),2)`\\
CoveragePN  & Log coverage amount of PN claim in mm  &  `r round(min(train$CoveragePN),2)` & `r round(mean(train$CoveragePN),2)`
            & `r round(max(train$CoveragePN),2)`\\
\hline \hline
\end{tabular}}
\end{center}
\end{table}

\begin{table}[h!t!]
\begin{center}
\caption{Summary statistics for claim frequency} \label{tab:2}
\resizebox{!}{1.2cm}{
\begin{tabular}{l|lrrrr}
\hline \hline
        &                               & Minimum & Mean & Variance & Maximum \\
\hline
FreqBC  & number of BC claim in a year  & `r round(min(train$FreqBC),2)` &
`r round(mean(train$FreqBC),2)`         & `r round(var(train$FreqBC),2)` &
`r round(max(train$FreqBC),2)` \\
FreqIM  & number of IM claim in a year  & `r round(min(train$FreqIM),2)` &
`r round(mean(train$FreqIM),2)`         & `r round(var(train$FreqIM),2)` &
`r round(max(train$FreqIM),2)` \\
FreqPN  & number of PN claim in a year  & `r round(min(train$FreqPN),2)` &
`r round(mean(train$FreqPN),2)`         & `r round(var(train$FreqPN),2)` &
`r round(max(train$FreqPN),2)` \\
\hline \hline
\end{tabular}}
\end{center}
\end{table}

In terms of frequency, IM has relatively moderate dispersion of the number of claim per year, whereas BC has very wide range. Usually, dataset used to calibrate two-parts GLM in practice rarely contains a policy which has more than six claims in a year. So we may need a different methodology for modelling such unusual high frequency.

```{r, echo=FALSE, message=FALSE,warning=FALSE}
library(knitr)
library(kableExtra)
library(plyr)
library(fitdistrplus)

Fumm <- data.frame(train$FreqBC)
colnames(Fumm) <- "Freq"
freqtable <- as.data.frame(count(Fumm, 'Freq'))
colnames(freqtable) <- c("Count","BC")
ft <- rbind( freqtable[1:10, ],c(">9",
                                          sum(freqtable$BC[11:length(freqtable$BC)])) )

Fumm <- data.frame(train$FreqIM)
colnames(Fumm) <- "Freq"
freqtable <- as.data.frame(count(Fumm, 'Freq'))
colnames(freqtable) <- c("Count","IM")
freqtable <- rbind(freqtable,c(0,0))
freqtable <- rbind(freqtable,c(0,0))
freqtable <- rbind(freqtable,c(0,0))
freqtable <- rbind(freqtable,c(0,0))
ft <- cbind(ft,freqtable$IM)
colnames(ft)[3] <- "IM"

Fumm <- data.frame(train$FreqPN)
colnames(Fumm) <- "Freq"
freqtable <- as.data.frame(count(Fumm, 'Freq'))
colnames(freqtable) <- c("Count","PN")
freqtable <- rbind( freqtable[1:10, ],c(">9",
                                 sum(freqtable$PN[11:length(freqtable$PN)])) )

ft <- cbind(ft,freqtable$PN)
colnames(ft)[4] <- "PN"
options(knitr.table.format = "latex")
kable_styling(kable(ft,caption = "Distribution of frequency per claim type",booktabs = T,digits=0,linesep = c("", "", "", "", "","", "", "", "", "", "", "\\hline"),  toprule = "\\hhline{====}",bottomrule="\\hhline{====}",escape=FALSE)  %>%  add_header_above(c(" " = 2, "Fitted" = 2)), latex_options = "hold_position")
```

\begin{table}[h!t!]
\begin{center}
\caption{Summary statistics for claim severity} \label{tab:3}
\resizebox{!}{1.08cm}{
\begin{tabular}{l|lrrrr}
\hline \hline
        &                               & Minimum & Mean & Variance & Maximum \\
\hline
log(yAvgBC)  & (log) avg size of BC claim in a year  & `r round(min(log(train$yAvgBC[train$yAvgBC>1])),2)` &
`r round(mean(log(train$yAvgBC[train$yAvgBC>1])),2)`         & `r round(var(log(train$yAvgBC[train$yAvgBC>1])),2)` &
`r round(max(log(train$yAvgBC[train$yAvgBC>1])),2)` \\
log(yAvgIM)  & (log) avg size of IM claim in a year  & `r round(min(log(train$yAvgIM[train$yAvgIM>1])),2)` &
`r round(mean(log(train$yAvgIM[train$yAvgIM>1])),2)`         & `r round(var(log(train$yAvgIM[train$yAvgIM>1])),2)` &
`r round(max(log(train$yAvgIM[train$yAvgIM>1])),2)` \\
log(yAvgPN)  & (log) avg size of PN claim in a year  & `r round(min(log(train$yAvgPN[train$yAvgPN>1])),2)` &
`r round(mean(log(train$yAvgPN[train$yAvgPN>1])),2)`         & `r round(var(log(train$yAvgPN[train$yAvgPN>1])),2)` &
`r round(max(log(train$yAvgPN[train$yAvgPN>1])),2)` \\
\hline \hline
\end{tabular}}
\end{center}
\end{table}

\subsection{Model Specification}
For prediction, I applied two type of model, one is likelihood based estimation which includes traditional Poisson-gamma two parts model, and the other is the use of neural network. For frequency part, first I fitted the model with Poisson, zero-inflated Poisson (ZIP), and negative binomial (NB). Since I found that frequency fit with Poisson was best both for BC and IM claim whereas ZIP outperformed all the other candidates in PN claim, I used Poisson in BC and IM, and ZIP in PN, respectively. For severity part, I used usual gamma, generalized pareto (GP), and generalized beta of the second kind (GB2) distribution, which was also used in @yang2011gb2. Note that GP and GB2 distribution could be derived under the random effects framework for repeated measurement per each policyholder.

More specifically, suppose gamma/inv-gamma random effect model is given as following.
$$
Y_t|U \sim \  \text{Gamma}({\psi_t}, U   \frac{\mu_{t}}{\psi_{t}} ) \quad \text{and} \quad U \sim \text{Inv-Gamma}(\eta+1,\eta)
$$
Then We can derive a multivariate joint distribution of $\textbf{Y}_T = (Y_1,Y_2,\ldots,Y_T)'$ by integrating out the random effects $U$.
$$
\begin{aligned}
f_{\textbf{Y}_T}(\textbf{y}_T)&=\int_0 ^{\infty} \prod_{t=1}^T f_{Y_t|U}(y_t|u)p(u)du\\
&=\frac{  \eta^{\eta+1} \prod_{t=1}^T (\psi_t y_t \mu_t^{-1} )^{\psi_t} } {(\eta+\sum_{t=1}^T \psi_t y_t \mu_t^{-1} )^{\sum\psi_t +\eta+1}} \times \frac{  \Gamma(\sum\psi_t+\eta+1)\prod_{t=1}^T y_t^{-1}}{\prod_{t=1}^T \Gamma(\psi_t)\Gamma(\eta+1)}
\end{aligned}
$$
Now, using given joint density, we may derive conditonal distribution of $Y_{T+1}$ given $\textbf{Y}_T$. Here, let us denote $w_T = \eta+\sum_{t=1}^{T}\psi_ty_t\mu_t^{-1}$, and $\eta_T=\eta+\sum_{t=1}^{T}\psi_t$.
$$
\begin{aligned}
f_{Y_{T+1}|\textbf{Y}_T}(y_{T+1}|\textbf{y}_T)&= f_{\textbf{Y}_{T+1}}(\textbf{y}_{T+1} ) / f_{\textbf{Y}_T}(\textbf{y}_T) \\
&=\frac{  w_T^{\eta+1} (\psi_{T+1} y_{T+1} \mu_{T+1}^{-1} )^{\psi_{T+1}} } {(w_T +\psi_{T+1} y_{T+1} \mu_{T+1}^{-1} )^{\psi_{T+1} +\eta_T+1}}  \times \frac{  \Gamma(\psi_{T+1}+\eta_T+1)y_{T+1}^{-1}}{ \Gamma(\psi_{T+1})\Gamma(\eta_T+1)}
\end{aligned}
$$
As a result, we can see that $Y_{T+1} | \textbf{Y}_T \sim GP(\eta_T+1,  w_T{\mu_{T+1}  /  \psi_{T+1}},\psi_{T+1})$ and $\E{Y_{T+1} | \textbf{Y}_T}=\frac{w_T\mu_{T+1}\psi_{T+1}}{(\eta_T+1-1)\psi_{T+1}}=\frac{w_T}{\eta_T} \mu_{T+1}$

Note that we may use the previous argument for the average severity modelling by denoting
$$
Y_t = \overline{C}_t|N_t, \ \psi_t = N_t/\phi, \ \mu_t = \exp(X_t\beta + N_t\theta), \ \eta= k/\phi
$$
Therefore, we have two types of premium, a priori premium and a posteriori premium, which is a product of weight factor from previous observation and a priori premium under GP distribution.
$$
\begin{aligned}
\E{\overline{C}_{T+1} |N_{T+1} }&= \exp(X_{T+1}\beta + N_{T+1}\theta) \\
\E{\overline{C}_{T+1} | \overline{\textbf{C}}_T, \textbf{N}_T }&= \exp(X_{T+1}\beta + N_{T+1}\theta) \frac{k+\sum_{t=1}^{T}S_t\mu_t^{-1}}{k+\sum_{t=1}^{T}N_t} 
\end{aligned}
$$
Likewise, we may derive GB2 distribution from random effects framework. Let us assume that G-Gamma/GI-gamma random effect model is given as following. We denote that $z_t=\frac{\Gamma(\psi_t+1/p)}{\Gamma(\psi_t)}$, and $w=\frac{\Gamma(\eta+1)}{\Gamma(\eta+1-1/p)}$.
$$
Y_t|U \sim \  \text{G-Gamma}({\psi_t}, U   \frac{\mu_{t}}{z_{t}},p ) \quad \text{and} \quad U \sim \text{GI-Gamma}(\eta+1,w,p)
$$
Then we can derive a multivariate joint distribution of $\textbf{Y}_T = (Y_1,Y_2,\ldots,Y_T)'$ by integrating out the random effects $U$ as well.
$$
\begin{aligned}
f_{\textbf{Y}_T}(\textbf{y}_T)&=\int_0 ^{\infty} \prod_{t=1}^T f_{Y_t|U}(y_t|u)p(u)du\\
&=\frac{ p^T w^{p(\eta+1)} \prod_{t=1}^T (z_t y_t \mu_t^{-1} )^{p\psi_t} } {(w^p+\sum_{t=1}^T (z_t y_t \mu_t^{-1})^p )^{\sum\psi_t +\eta+1}} \times \frac{  \Gamma(\sum\psi_t+\eta+1)\prod_{t=1}^T y_t^{-1}}{\prod_{t=1}^T \Gamma(\psi_t)\Gamma(\eta+1)}
\end{aligned}
$$
Now, using given joint density, we may derive conditonal distribution of $Y_{T+1}$ given $\textbf{Y}_T$. Here, let us denote $w^{*}_{T,p} = \sqrt[p] {w^p+\sum_{t=1}^{T} (\psi_ty_t\mu_t^{-1})^p}$, and $\eta_T=\eta+\sum_{t=1}^{T}\psi_t$, then we can get
$$
\begin{aligned}
f_{Y_{T+1}|\textbf{Y}_T}(y_{T+1}|\textbf{y}_T)&= f_{\textbf{Y}_{T+1}}(\textbf{y}_{T+1} ) / f_{\textbf{Y}_T}(\textbf{y}_T) \\
&=\frac{  (w^{*}_{T,p})^{p(\eta+1)} (z_{T+1} y_{T+1} \mu_{T+1}^{-1} )^{p\psi_{T+1}} } {((w^{*}_{T,p})^p +(z_{T+1} y_{T+1} \mu_{T+1}^{-1})^p )^{\psi_{T+1} +\eta_T+1}} \times \frac{  \Gamma(\psi_{T+1}+\eta_T+1)y_{T+1}^{-1}}{ \Gamma(\psi_{T+1})\Gamma(\eta_T+1)}.
\end{aligned}
$$
As a result, we can see that $Y_{T+1} | \textbf{Y}_T \sim GB2(\eta_T+1,  w^*_{T,p}{\mu_{T+1}  /  z_{T+1}},\psi_{T+1},p)$ 
so that $\E{Y_{T+1} | \textbf{Y}_T}=w^*_{T,p}\mu_{T+1}\frac{\Gamma(\eta_T+1-1/p)z_{T+1}}{\Gamma(\eta_T+1)z_{T+1}}=w^*_{T,p}\frac{\Gamma(\eta_T+1-1/p)}{\Gamma(\eta_T+1)} \mu_{T+1}$

Again, we may use the previous argument for the average severity modelling by denoting
$$
Y_t = \overline{C}_t|N_t, \ \psi_t = N_t/\phi, \ \mu_t = \exp(X_t\beta + N_t\theta), \ \eta = k/\phi
$$
Therefore, we have two types of premium, a priori premium and a posteriori premium as well.
$$
\begin{aligned}
\E{\overline{C}_{T+1} |N_{T+1} }&= \exp(X_{T+1}\beta + N_{T+1}\theta) \\
\E{\overline{C}_{T+1} | \overline{\textbf{C}}_T, \textbf{N}_T }&= \exp(X_{T+1}\beta + N_{T+1}\theta) \times  \sqrt[p] {w^p+\sum_{t=1}^{T} (\overline{C}_t\mu_t^{-1} z_t )^p}\frac{\Gamma(k/\phi+1+\sum_{t=1}^{T}N_t/\phi-1/p)}{\Gamma(k/\phi+1+\sum_{t=1}^{T}N_t/\phi)}
\end{aligned}
$$
For a posteriori premium, We may observe that as $k \rightarrow \infty$, the following holds.
$$
\begin{aligned}
& \frac{k+\sum_{t=1}^{T}S_t\mu_t^{-1}}{k+\sum_{t=1}^{T}N_t} \ \rightarrow \ 1, \qquad  \sqrt[p] {w^p+\sum_{t=1}^{T} (\overline{C}_t\mu_t^{-1} z_t )^p}\frac{\Gamma(k/\phi+1+\sum_{t=1}^{T}N_t/\phi-1/p)}{\Gamma(k/\phi+1+\sum_{t=1}^{T}N_t/\phi)} \ \rightarrow \ 1 \\
& \left( \because \ {\displaystyle \lim _{n\to \infty }{\frac {\Gamma (n+\alpha )}{\Gamma (n)n^{\alpha }}}=1,\quad \alpha \in \mathbb {C} }  \ \text{ and } \  w=\frac{\Gamma(k/\phi+1)}{\Gamma(k/\phi+1-1/p)} \right)
\end{aligned}
$$
Therefore, $k$ works as a smoothing factor for a posteriori premium. In other words, if we choose very small $k$, then we use more information from the past, whereas if we choose relatively large $k$, then we use less information from the past. I chose optimal $k$ for each claim using cross-validation in training set and the following is the distribution of weight factors for each claim with optimal $k$. 

```{r,echo=FALSE}
load(file="BC.Rdata")
wf <- rbind(summary(postweight),summary(post2weight))
rownames(wf)[1:2] <- c("BC: GP weight","BC: GB2 weight")
load(file="IM.Rdata")
wf <- rbind(wf,summary(postweight),summary(post2weight))
rownames(wf)[3:4] <- c("IM: GP weight","IM: GB2 weight")
load(file="PN.Rdata")
wf <- rbind(wf,summary(postweight),summary(post2weight))
rownames(wf)[5:6] <- c("PN: GP weight","PN: GB2 weight")

kable_styling(kable(wf,caption = "Distribution of weight factors for each claim",booktabs = T,digits=3,linesep = c("", "", "", "", "","", "", "", "", "", "", "\\hline"),  toprule = "\\hhline{=======}",bottomrule="\\hhline{=======}",escape=FALSE), latex_options = "hold_position")
```

Furthermore, I used neural network for both two-parts estimation and direct compound loss estimation. Note that I excluded tweedie distribution since it showed relatively poor performance in terms of validation measures. The following is brief summary for the entertained models. Note that I did not include Tweedie model as a candidate because it underperformed all the other listed models according to a preliminary analysis.

\begin{itemize}
  \item Likelihood based models
  \begin{itemize}
    \item Frequency part: Poisson in BC and IM, Zero-inflated
    \item Severity part: Gamma, GP (Prior/Posterior), and GB2 (Prior/Posterior)
  \end{itemize}
  \item Neural network for two-parts and compound loss
  \item Naive method: sample mean of compound loss in training set
\end{itemize}

\newpage

\subsection{Estimation and Prediction}

For validation measure, I used Gini index and mean squared error (MSE). Gini index, originated from economical concept describing the distribution of wealth among people, is used to distribution of 'risk' among the insured. For detailed explanation, see @frees2014gini. I drew the Lorenz curve with the following three-step process computed the corresponding Gini index.

\begin{enumerate}
  \item From our hold-out sample, for $i=1,2,\ldots,M$, sort the observed loss $Y_{i}$ according to the risk score ${S_i}$ for which in our case, the predicted value from each model, in an ascending manner. That is, calculate the rank $R_i$ of $S_i$ between 1 and M with $R_1 = \text{argmin}(S_i)$.
	\item Compute $F_{\text{score}}(m/M) = \dfrac{1}{M} \sum_{i=1}^{M}{1_{(R_i \leq m)}}$, the cumulative percentage of exposures, and $F_{\text{loss}(m/M)} = \dfrac{\sum_{i=1}^{M} Y_i 1_{(R_i \leq m)}}{\sum_{i=1}^{M} Y_i}$, the cumulative percentage of loss, for each $m = 1,2,\ldots,M$.
	\item Plot $F_{\text{score}}(m/M)$ on the $x$-axis, and $F_{\text{loss}}(m/M)$ on the $y$-axis.
\end{enumerate}

As we can see, Lorenz curve and Gini index depends only on the `rank' so if there are many observed loss which have the same value, then it might be distorted. Therefore, I used only positive amounts of actual loss (and corresponding predicted pure premium) for drawing Lorenz curves to avoid that issue.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
setwd("C:/Users/HimChan/Desktop/DAction")
source("Gindex.R")
load(file="BC.Rdata")
nv <- rep(mean(train$ClaimBC),nrow(test))
MSE_gam      <- sqrt(mean((glm.avgsev.pois_pred*glm.freq.pois_pred - test$ClaimBC)^2)) # two-part glm (pois + gamma)
MSE_nnet2dep <- sqrt(mean((nnet.avgsev_dep.pred*nnet.freq.pred - test$ClaimBC)^2)) # two-part nnet
MSE_prigp    <- sqrt(mean((gp.avgsev.pois_pred*glm.freq.pois_pred - test$ClaimBC)^2)) # priori two-part (pois + gp)
MSE_posgp    <- sqrt(mean((gp.avgsev.pois_pred*glm.freq.pois_pred*postweight - test$ClaimBC)^2)) # posterior two-part (pois + gp)
MSE_prigb2   <- sqrt(mean((GB2.avgsev.pois_pred  *glm.freq.pois_pred - test$ClaimBC)^2)) # dep two-part glm (pois + GB2)
MSE_posgb2   <- sqrt(mean((GB2.avgsev.pois_pred  *glm.freq.pois_pred*post2weight - test$ClaimBC)^2)) # dep two-part glm (pois + GB2)
MSE_nnet1    <- sqrt(mean((nnet.tsev.pred - test$ClaimBC)^2))    # one-part nnet
MSE_naive    <- sqrt(mean((nv - test$ClaimBC)^2)) # naive

MSE <- as.data.frame(c(MSE_gam,MSE_nnet2dep,MSE_prigp,MSE_posgp,MSE_prigb2,MSE_posgb2,MSE_nnet1,MSE_naive))
colnames(MSE) <- "BC"
rownames(MSE) <- c("Gamma","2P NN","Prior GP","Posterior GP","Prior GB2","Posterior GB2","1P NN","Naive")
```

```{r, echo=FALSE, message=FALSE, gini_BC, fig.cap="The Lorenz curve and the Gini index values for BC claim",fig.height=6.15,fig.width=10}
Px <- rep(1,length(test$ClaimBC[test$ClaimBC>0]))
par(mfrow=c(2,4),mar=c(1,1,4,1),mgp=c(2.2,1,0),oma=c(0,0,0,0))
gingraph.poisgam_dep <- gini.index.graphic(glm.avgsev.pois_pred[test$ClaimBC>0]*glm.freq.pois_pred[test$ClaimBC>0]
                                         ,test$ClaimBC[test$ClaimBC>0],Px,Title="Dep Pois-Gamma GLM")
gingraph.poisgp_pri <- gini.index.graphic(gp.avgsev.pois_pred[test$ClaimBC>0]*glm.freq.pois_pred[test$ClaimBC>0]
                                         ,test$ClaimBC[test$ClaimBC>0],Px,Title="Pois-GP Prior")
gingraph.poisgp_pos <- gini.index.graphic((gp.avgsev.pois_pred*glm.freq.pois_pred*postweight)[test$ClaimBC>0]
                                         ,test$ClaimBC[test$ClaimBC>0],Px,Title="Pois-GP Posterior")
gingraph.poisgb2_pri <- gini.index.graphic(GB2.avgsev.pois_pred[test$ClaimBC>0]*glm.freq.pois_pred[test$ClaimBC>0]
                                         ,test$ClaimBC[test$ClaimBC>0],Px,Title="Pois-GB2 Prior")
gingraph.poisgb2_pos <- gini.index.graphic((GB2.avgsev.pois_pred*glm.freq.pois_pred*post2weight)[test$ClaimBC>0]
                                         ,test$ClaimBC[test$ClaimBC>0],Px,Title="Pois-GB2 Posterior")
gingraph.2nnet_dep <- gini.index.graphic(nnet.avgsev_dep.pred[test$ClaimBC>0]*nnet.freq.pred[test$ClaimBC>0]
                                         ,test$ClaimBC[test$ClaimBC>0],Px,Title="Dep Two-parts NN")
gingraph.1nnet <- gini.index.graphic(nnet.tsev.pred[test$ClaimBC>0],test$ClaimBC[test$ClaimBC>0],Px,Title="One-part NN")
gingraph.naive <- gini.index.graphic(nv[test$ClaimBC>0],Px,Px,Title="Naive sample mean")
```

We can see that Gini index for naive model is 0, since we charge the same premium for all policyholder so that there is no risk classification. And one-part neural network showed the best performance in terms of Gini indices in BC and IM claim, while the use of Poisson-GP prior two parts model show the same performance in case of IM claim. Note that for PN claim, there are little differences among the PN models in Gini indices, which might be due to the lack of relevant covariates. Usually, driver's gender, age, vehicle type and capacity have significant effects on the automobile insurance claim and severity. However, in the dataset, there are only regional information which cannot be enough for risk classification of automobile insurance.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
load(file="IM.Rdata")
nv <- rep(mean(train$ClaimIM),nrow(test))
MSE_gam      <- sqrt(mean((glm.avgsev.pois_pred*glm.freq.pois_pred - test$ClaimIM)^2)) # two-part glm (pois + gamma)
MSE_nnet2dep <- sqrt(mean((nnet.avgsev_dep.pred*nnet.freq.pred - test$ClaimIM)^2)) # two-part nnet
MSE_prigp    <- sqrt(mean((gp.avgsev.pois_pred*glm.freq.pois_pred - test$ClaimIM)^2)) # prior two-part (pois + gp)
MSE_posgp    <- sqrt(mean((gp.avgsev.pois_pred*glm.freq.pois_pred*postweight - test$ClaimIM)^2)) # posterior two-part (pois + gp)
MSE_prigb2   <- sqrt(mean((GB2.avgsev.pois_pred  *glm.freq.pois_pred - test$ClaimIM)^2)) # dep two-part glm (pois + GB2)
MSE_posgb2   <- sqrt(mean((GB2.avgsev.pois_pred  *glm.freq.pois_pred*post2weight - test$ClaimIM)^2)) # dep two-part glm (pois + GB2)
MSE_nnet1    <- sqrt(mean((nnet.tsev.pred - test$ClaimIM)^2))    # one-part nnet
MSE_naive    <- sqrt(mean((nv - test$ClaimIM)^2)) # naive
MSE$IM <- c(MSE_gam,MSE_nnet2dep,MSE_prigp,MSE_posgp,MSE_prigb2,MSE_posgb2,MSE_nnet1,MSE_naive)
```


```{r, echo=FALSE, message=FALSE, gini_IM, fig.cap="The Lorenz curve and the Gini index values for IM claim",fig.height=6.15,fig.width=10}
source("Gindex.R")
Px <- rep(1,length(test$ClaimIM[test$ClaimIM>0]))
par(mfrow=c(2,4),mar=c(1,1,4,1),mgp=c(2.2,1,0),oma=c(0,0,0,0))
gingraph.poisgam_dep <- gini.index.graphic(glm.avgsev.pois_pred[test$ClaimIM>0]*glm.freq.pois_pred[test$ClaimIM>0]
                                         ,test$ClaimIM[test$ClaimIM>0],Px,Title="Dep Pois-Gamma GLM")
gingraph.poisgp_pri <- gini.index.graphic(gp.avgsev.pois_pred[test$ClaimIM>0]*glm.freq.pois_pred[test$ClaimIM>0]
                                         ,test$ClaimIM[test$ClaimIM>0],Px,Title="Pois-GP Prior")
gingraph.poisgp_pos <- gini.index.graphic((gp.avgsev.pois_pred*glm.freq.pois_pred*postweight)[test$ClaimIM>0]
                                         ,test$ClaimIM[test$ClaimIM>0],Px,Title="Pois-GP Posterior")
gingraph.poisgb2_pri <- gini.index.graphic(GB2.avgsev.pois_pred[test$ClaimIM>0]*glm.freq.pois_pred[test$ClaimIM>0]
                                         ,test$ClaimIM[test$ClaimIM>0],Px,Title="Pois-GB2 Prior")
gingraph.poisgb2_pos <- gini.index.graphic((GB2.avgsev.pois_pred*glm.freq.pois_pred*post2weight)[test$ClaimIM>0]
                                         ,test$ClaimIM[test$ClaimIM>0],Px,Title="Pois-GB2 Posterior")
gingraph.2nnet_dep <- gini.index.graphic(nnet.avgsev_dep.pred[test$ClaimIM>0]*nnet.freq.pred[test$ClaimIM>0]
                                         ,test$ClaimIM[test$ClaimIM>0],Px,Title="Dep Two-parts NN")
gingraph.1nnet <- gini.index.graphic(nnet.tsev.pred[test$ClaimIM>0],test$ClaimIM[test$ClaimIM>0],Px,Title="One-part NN")
gingraph.naive <- gini.index.graphic(nv[test$ClaimIM>0],Px,Px,Title="Naive sample mean")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
load(file="PN.Rdata")
nv <- rep(mean(train$ClaimPN),nrow(test))
MSE_gam      <- sqrt(mean((glm.avgsev.zip_pred*glm.freq.zip_pred - test$ClaimPN)^2)) # two-part glm (zip + gamma)
MSE_nnet2dep <- sqrt(mean((nnet.avgsev_dep.pred*nnet.freq.pred - test$ClaimPN)^2)) # two-part nnet
MSE_prigp    <- sqrt(mean((gp.avgsev.zip_pred*glm.freq.zip_pred - test$ClaimPN)^2)) # prior two-part (zip + gp)
MSE_posgp    <- sqrt(mean((gp.avgsev.zip_pred*glm.freq.zip_pred*postweight - test$ClaimPN)^2)) # posterior two-part (zip + gp)
MSE_prigb2   <- sqrt(mean((GB2.avgsev.zip_pred  *glm.freq.zip_pred - test$ClaimPN)^2)) # priori two-part glm (zip + GB2)
MSE_posgb2   <- sqrt(mean((GB2.avgsev.zip_pred  *glm.freq.zip_pred*post2weight - test$ClaimPN)^2)) # dep two-part glm (zip + GB2)
MSE_nnet1    <- sqrt(mean((nnet.tsev.pred - test$ClaimPN)^2))    # one-part nnet
MSE_naive    <- sqrt(mean((nv - test$ClaimPN)^2)) # naive
MSE$PN <- c(MSE_gam,MSE_nnet2dep,MSE_prigp,MSE_posgp,MSE_prigb2,MSE_posgb2,MSE_nnet1,MSE_naive)
```

```{r, echo=FALSE, message=FALSE, gini_PN, fig.cap="The Lorenz curve and the Gini index values for PN claim",fig.height=6.15,fig.width=10}
source("Gindex.R")
Px <- rep(1,length(test$ClaimPN[test$ClaimPN>0]))
par(mfrow=c(2,4),mar=c(1,1,4,1),mgp=c(2.2,1,0),oma=c(0,0,0,0))
gingraph.zipgam_dep <- gini.index.graphic(glm.avgsev.zip_pred[test$ClaimPN>0]*glm.freq.zip_pred[test$ClaimPN>0]
                                         ,test$ClaimPN[test$ClaimPN>0],Px,Title="Dep ZIP-Gamma GLM")
gingraph.zipgp_pri <- gini.index.graphic(gp.avgsev.zip_pred[test$ClaimPN>0]*glm.freq.zip_pred[test$ClaimPN>0]
                                         ,test$ClaimPN[test$ClaimPN>0],Px,Title="ZIP-GP Prior")
gingraph.zipgp_pos <- gini.index.graphic((gp.avgsev.zip_pred*glm.freq.pois_pred*postweight)[test$ClaimPN>0]
                                         ,test$ClaimPN[test$ClaimPN>0],Px,Title="ZIP-GP Posterior")
gingraph.zipgb2_pri <- gini.index.graphic(GB2.avgsev.zip_pred[test$ClaimPN>0]*glm.freq.zip_pred[test$ClaimPN>0]
                                         ,test$ClaimPN[test$ClaimPN>0],Px,Title="ZIP-GB2 Prior")
gingraph.zipgb2_pos <- gini.index.graphic((GB2.avgsev.zip_pred*glm.freq.pois_pred*post2weight)[test$ClaimPN>0]
                                         ,test$ClaimPN[test$ClaimPN>0],Px,Title="ZIP-GB2 Posterior")
gingraph.2nnet_dep <- gini.index.graphic(nnet.avgsev_dep.pred[test$ClaimPN>0]*nnet.freq.pred[test$ClaimPN>0]
                                         ,test$ClaimPN[test$ClaimPN>0],Px,Title="Dep Two-parts NN")
gingraph.1nnet <- gini.index.graphic(nnet.tsev.pred[test$ClaimPN>0],test$ClaimPN[test$ClaimPN>0],Px,Title="One-part NN")
gingraph.naive <- gini.index.graphic(nv[test$ClaimPN>0],Px,Px,Title="Naive sample mean")

```

\newpage

```{r, echo=FALSE, message=FALSE}
kable_styling(kable(MSE,caption = "MSEs for all type of claim per each model",booktabs = T,digits=0,linesep = c("", "", "", "", "","", "", "", "", "", "", "\\hline"),  toprule = "\\hhline{====}",bottomrule="\\hhline{====}",escape=FALSE), latex_options = "hold_position")
```

Sample mean is the most naive and simple estimator but sometimes it is hard to outperform that even with so-called `sophisticated method' - and that is why insurance companies try to increase market share continually. According to the MSEs of given models, in BC and IM claims there were no big differences among the MSE of naive method, two-parts neural network, and one-part neural network. However, we can see that either posterior GP or posterior GB2 models outperformed naive method and neural network in all types of claim with moderate risk classification level - measured by Gini index. Therefore, I can claim that use of posteriori premium based on GP or GB2 distribution is the best among the entertained models.

\section{Concluding Remarks}

I could show that with the presence of relevent covariates, use of posterior GB2 distribution showed good performance for the building and contents (BC) claim prediction even with unusual claim feature - very high claim frequency per year. Moreover, in the use of MVGB2 distribution, parameter $k$ works as a regularizing parameter so that $k=\infty$ and $p=1$ is equivalent to current i.i.d. gamma GLM framework for the average severity. Therefore, proposed MVGB2 is a natural extension of current two-parts model entertained in most of P\&C insurance company, which can add the more complexity while retaining interpretability of the model.

As future works, it would be worthwhile to calibrate auto insurance claim with the posterior GB2 distribution, upon the existence of relevant explanatory variables. Furthermore, when deriving GB2 distribution the unit of repetead measurement needs not be limited to each policyholder, but might be the classes of policyholder with the same bonus-malus score, or certain risk homogeneous classes obtained by clustering methods.