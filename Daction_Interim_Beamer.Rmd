---
title: "Predictive models for P&C insurance"
author: "Himchan Jeong"
date: "10 April, 2018"
output:
  beamer_presentation:
    keep_tex: true
    colortheme: orchid
    slide_level: 1
    template: default.tex
    theme: madrid
  slidy_presentation: default
institute: University of Connecticut
place: Data Science in Action
shorttitle: 'STAT 6494: Data Science in Action'
header-includes:
- \usepackage{bbm}
- \usepackage{xcolor}
- \usepackage{hhline}
- \newcommand{\E}[1]{{\mathbb E}\left[#1\right]}
- \newcommand{\Var}[1]{{\mathrm Var}\left(#1\right)}
- \usepackage{beamerthemesplit}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{caption}
- \newcommand\tab[1][1cm]{\hspace*{#1}}
- \captionsetup[table]{belowskip=0pt,aboveskip=0pt}
- \captionsetup[figure]{belowskip=0pt,aboveskip=0pt}
---

# Purpose of the Project
\begin{itemize}
\item Introduce current practice done by property and casualty (P\&C) insurance company
\item Suggest the more sophisticated predictive model which can outperform the benchmarks
\end{itemize}

# Current Approches for Claim Modeling
\begin{itemize}
\item (1) Two-parts model for frequency and severity
\item (2) Tweedie model
\end{itemize}

# Pitfalls in Current Practices
\begin{itemize}
\item (1) Dependence between the frequency and the severity
\smallskip
\item (2) Longitudinal property of data structure. 
  \begin{itemize}
  \item For example, if we observed a policyholder $i$ for $T_i$ years, then we have following observation $N_{i1},N_{i2},\ldots,N_{iT_i}$, which may not be identically and independently distributed.
  \end{itemize}
\end{itemize}

# Possible Alternatives for the Benchmarks
\begin{itemize}
  \item For dependence between the frequency and severity
    \begin{itemize}
    \item \color{purple}Set $\E{\overline{C}|N}=e^{X\beta+N\theta}$
    \end{itemize}
\smallskip \color{black}
  \item For longitudinal property
    \begin{itemize}
    \item Random effects model
    \end{itemize}
\smallskip
  \item Non-traditional approaches
    \begin{itemize} \color{purple}
    \item Neural network \color{black}
    \item Regression for each group classified by decision tree
    \end{itemize}
\end{itemize}

```{r, message=FALSE, echo=FALSE}
setwd("C:/Users/HimChan/Desktop/DAction")
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

# Data Description
\begin{itemize}
  \item Here I use a public dataset on insurance claim, provided by Wisconsin Propery Fund. \newline
        (https://sites.google.com/a/wisc.edu/jed-frees/)
  \item It consists of `r prettyNum(length(train$PolicyNum),big.mark=",")` observation in traning set and `r prettyNum(length(test$PolicyNum),big.mark=",")` observation in test set.
  \item It is a longitudinal data with more or less `r prettyNum(length(unique(train$PolicyNum)),big.mark=",")` policyholder, followed for `r max(train$Year) - min(train$Year)+1` years.
  \item Since the dataset includes information on multi-line insurance, here I used building and contents (BC), inland marine (IM), and new motor vehicle (PN) claim information.
\end{itemize}

# Observable Policy Characteristics used as Covariates
\begin{center}
\resizebox{!}{3.35cm}{
\begin{tabular}{l|lrrr}
\hline \hline
Categorical & Description &  & \multicolumn{2}{c}{Proportions} \\
variables \\
\hline
TypeCity & Indicator for city entity:           & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeCity)/length(train$PolicyNum),2)` \%} \\
TypeCounty & Indicator for county entity:       & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeCounty)/length(train$PolicyNum),2)` \%} \\
TypeMisc & Indicator for miscellaneous entity:  & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeMisc)/length(train$PolicyNum),2)` \%} \\
TypeSchool & Indicator for school entity:       & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeSchool)/length(train$PolicyNum),2)` \%} \\
TypeTown & Indicator for town entity:           & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeTown)/length(train$PolicyNum),2)` \%} \\
TypeVillage & Indicator for village entity:     & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$TypeVillage)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditBC & No BC claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditBC)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditIM & No IM claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditIM)/length(train$PolicyNum),2)` \%} \\
NoClaimCreditPN & No PN claim in prior year:    & Y=1 & \multicolumn{2}{c}{`r round(100*sum(train$NoClaimCreditPN)/length(train$PolicyNum),2)` \%} \\

\hline
 Continuous & & Minimum & Mean & Maximum \\
 variables \\
\hline
CoverageBC  & Log coverage amount of BC claim in mm  &  `r round(min(train$CoverageBC),2)` & `r round(mean(train$CoverageBC),2)`
            & `r round(max(train$CoverageBC),2)`\\
lnDeductBC  & Log deductible amount for BC claim     &  `r round(min(train$lnDeductBC),2)` & `r round(mean(train$lnDeductBC),2)`
            & `r round(max(train$lnDeductBC),2)`\\
CoverageIM  & Log coverage amount of IM claim in mm  &  `r round(min(train$CoverageIM),2)` & `r round(mean(train$CoverageIM),2)`
            & `r round(max(train$CoverageIM),2)`\\
lnDeductIM  & Log deductible amount for IM claim     &  `r round(min(train$lnDeductIM),2)` & `r round(mean(train$lnDeductIM),2)`
            & `r round(max(train$lnDeductIM),2)`\\
CoveragePN  & Log coverage amount of PN claim in mm  &  `r round(min(train$CoveragePN),2)` & `r round(mean(train$CoveragePN),2)`
            & `r round(max(train$CoveragePN),2)`\\
\hline \hline
\end{tabular}}
\end{center}

# Summary Statistics for Frequency

\begin{center}
\resizebox{!}{0.95cm}{
\begin{tabular}{l|lrrrr}
\hline \hline
        &                               & Minimum & Mean & Variance & Maximum \\
\hline
FreqBC  & number of BC claim in a year  & `r round(min(train$FreqBC),2)` &
`r round(mean(train$FreqBC),2)`         & `r round(var(train$FreqBC),2)` &
`r round(max(train$FreqBC),2)` \\
FreqIM  & number of IM claim in a year  & `r round(min(train$FreqIM),2)` &
`r round(mean(train$FreqIM),2)`         & `r round(var(train$FreqIM),2)` &
`r round(max(train$FreqIM),2)` \\
FreqPN  & number of PN claim in a year  & `r round(min(train$FreqPN),2)` &
`r round(mean(train$FreqPN),2)`         & `r round(var(train$FreqPN),2)` &
`r round(max(train$FreqPN),2)` \\
\hline \hline
\end{tabular}}
\end{center}

In terms of frequency, IM has relatively moderate dispersion of the number of claim per year, whereas BC has very wide range. Usually, dataset used to calibrate two-parts GLM in practice rarely contains a policy which has more than six claims in a year. So we may need a different methodology for modelling such unusual high frequency.

# Summary Statistics for Frequency (Cont'D)

```{r, echo=FALSE, message=FALSE,warning=FALSE}
library(knitr)
library(kableExtra)
library(plyr)
library(fitdistrplus)

Fumm <- data.frame(train$FreqBC)
colnames(Fumm) <- "Freq"
freqtable <- as.data.frame(count(Fumm, 'Freq'))
colnames(freqtable) <- c("Count","BC")
ft <- rbind( freqtable[1:10, ],c(">9",
                                          sum(freqtable$BC[11:length(freqtable$BC)])) )

Fumm <- data.frame(train$FreqIM)
colnames(Fumm) <- "Freq"
freqtable <- as.data.frame(count(Fumm, 'Freq'))
colnames(freqtable) <- c("Count","IM")
freqtable <- rbind(freqtable,c(0,0))
freqtable <- rbind(freqtable,c(0,0))
freqtable <- rbind(freqtable,c(0,0))
freqtable <- rbind(freqtable,c(0,0))
ft <- cbind(ft,freqtable$IM)
colnames(ft)[3] <- "IM"

Fumm <- data.frame(train$FreqPN)
colnames(Fumm) <- "Freq"
freqtable <- as.data.frame(count(Fumm, 'Freq'))
colnames(freqtable) <- c("Count","PN")
freqtable <- rbind( freqtable[1:10, ],c(">9",
                                 sum(freqtable$PN[11:length(freqtable$PN)])) )

ft <- cbind(ft,freqtable$PN)
colnames(ft)[4] <- "PN"
options(knitr.table.format = "latex")
kable_styling(kable(ft,caption = "Distribution of frequency per claim type",booktabs = T,digits=0,linesep = c("", "", "", "", "","", "", "", "", "", "", "\\hline"),  toprule = "\\hhline{====}",bottomrule="\\hhline{====}",escape=FALSE)  %>%  add_header_above(c(" " = 2, "Fitted" = 2)), latex_options = "hold_position")
```

# Summary Statistics for Severity

\begin{center}
\resizebox{!}{0.85cm}{
\begin{tabular}{l|lrrrr}
\hline \hline
        &                               & Minimum & Mean & Variance & Maximum \\
\hline
log(yAvgBC)  & (log) avg size of BC claim in a year  & `r round(min(log(train$yAvgBC[train$yAvgBC>1])),2)` &
`r round(mean(log(train$yAvgBC[train$yAvgBC>1])),2)`         & `r round(var(log(train$yAvgBC[train$yAvgBC>1])),2)` &
`r round(max(log(train$yAvgBC[train$yAvgBC>1])),2)` \\
log(yAvgIM)  & (log) avg size of IM claim in a year  & `r round(min(log(train$yAvgIM[train$yAvgIM>1])),2)` &
`r round(mean(log(train$yAvgIM[train$yAvgIM>1])),2)`         & `r round(var(log(train$yAvgIM[train$yAvgIM>1])),2)` &
`r round(max(log(train$yAvgIM[train$yAvgIM>1])),2)` \\
log(yAvgPN)  & (log) avg size of PN claim in a year  & `r round(min(log(train$yAvgPN[train$yAvgPN>1])),2)` &
`r round(mean(log(train$yAvgPN[train$yAvgPN>1])),2)`         & `r round(var(log(train$yAvgPN[train$yAvgPN>1])),2)` &
`r round(max(log(train$yAvgPN[train$yAvgPN>1])),2)` \\
\hline \hline
\end{tabular}}
\end{center}

# Entertained Models
\begin{itemize}
  \item Independent Two-parts [$\E{C|n}=\exp(X\beta)$]:
        
        Poisson-Gamma GLM, ZIP-Gamma GLM, neural network  
  \item Dependent Two-parts [$\E{C|n}=\exp(X\beta+n\theta)$]:
  
        Poisson-Gamma GLM, ZIP-Gamma GLM, neural network
  \item One-part [$\E{S}=\exp(X\eta)$]:
        
        Tweedie GLM, neural network
  
\end{itemize}

# Fitting Frequency in Neural Network

```{r, message=FALSE, echo=FALSE}
setwd("C:/Users/HimChan/Desktop/DAction")
train1 <- read.csv("train.csv")
train <- train1[,c(2:4,10:15,17:21)]
rm(train1)
test1 <- read.csv("test.csv")
test <- test1[,c(2:4,10:15,17:21)]
load(file="BC.Rdata")
```

```{r}
library(nnet)
ltFreq <- train$FreqBC
adjltFreq <- (ltFreq - min(ltFreq)) /
              (max(ltFreq)-min(ltFreq))
nnet.freq.fit<- nnet(adjltFreq~.,data=train[-c(1,2,3,6,13,14
                  )], size=10,linout=TRUE,trace=FALSE)
nnet.freq.pred <- predict(nnet.freq.fit, newdata = test
                  [-c(1,2,3,6,13,14)])*(max(ltFreq)
                  -min(ltFreq))+min(ltFreq)
```

# Fitting Average Severity in Neural Network

```{r}
trainp <- subset(train,log(yAvgBC)>0)
ltClaim <- log(trainp$yAvgBC)
adjltClaim <- (ltClaim - min(ltClaim))/
              (max(ltClaim)-min(ltClaim))
nnet.avgsev_dep.fit<- nnet(adjltClaim~.,data=trainp[-c(1,2,3,
                      6,13)],size=10,linout=TRUE,trace=FALSE)
nnet.avgsev_indep.fit <- nnet(adjltClaim~.,data=trainp
                        [-c(1,2,3,6,13,14)],size=10
                        ,linout=TRUE,trace=FALSE)
```

# Fitting Average Severity in Neural Network
 
```{r}
trainp <- subset(train,log(yAvgBC)>0)
ltClaim <- log(trainp$yAvgBC)
adjltClaim <- (ltClaim - min(ltClaim))/
              (max(ltClaim)-min(ltClaim))
nnet.avgsev_dep.fit<- nnet(adjltClaim~.,data=trainp[-c(1,2,3,
                      6,13)],size=10,linout=TRUE,trace=FALSE)
nnet.avgsev_indep.fit <- nnet(adjltClaim~.,data=trainp
                        [-c(1,2,3,6,13,14)],size=10
                        ,linout=TRUE,trace=FALSE)
```

# Fitting Aggregate Claim in Neural Network
 
```{r}
tClaim <- log(train$ClaimBC+1)
adjtClaim <- (tClaim - min(tClaim))/(max(tClaim)-min(tClaim))
nnet.tsev.fit <- nnet(adjtClaim~.,data=train[-c(1,2,3,6,
                 13,14)],size=5,linout=TRUE,trace=FALSE)
```

# Retrieving Prediction from fitted NN Model

When I got (a few) negative predictive values for frequency and total claim, I rounded up those values to 0.

```{r}
nnet.freq.pred <- predict(nnet.freq.fit, newdata =
                  test[-c(1,2,3,6,13,14)])*
                  (max(ltFreq)-min(ltFreq))+min(ltFreq)
nnet.freq.pred <- nnet.freq.pred *(nnet.freq.pred >0)

nnet.tsev.pred <- exp(predict(nnet.tsev.fit, newdata =
                  test[-c(1,2,3,6,13,14)])*
                  (max(tClaim)-min(tClaim)))-1
nnet.tsev.pred <- nnet.tsev.pred * (nnet.tsev.pred > 0)
```

# Retrieving Prediction from fitted NN Model (Cont'D)

In case of dependent two-part NN, we need to use $n$ as a covariate so we need to first estimate $n$ with fitted frequency model.

```{r}
glm.avgsev_indep.pred <- exp(predict(glm.avgsev_indep,
                          test[-c(1,2,3,6,13,14)]))
test.avg.pois <- cbind(test[-c(1,2,3,6,13,14)],
                  glm.freq.pois_pred)
colnames(test.avg.pois)[9] <- "FreqBC"
glm.avgsev.pois_pred <- exp(predict(glm.avgsev_dep,
                        test.avg.pois))
```

# Validation Measures for Model Comparison

\begin{itemize}
  \item Mean Squared Error
  \item Gini Index
    \begin{itemize}
      \item Gini index is equal to $2 \times$ the area between the line of equality and the Lorenz curve drawn below.
      \item In Lorenz curve, x-coordinate stands for cumulative proportion for number of policyholders, whereas
            y-coordinates stands for cumulative proportion of actual loss ordered by estimated premium.
      \item Therefore, it measures the ability of differentiation of risk per model.
    \end{itemize}
\end{itemize}

# Gini Indices for BC Claim
 
```{r, echo=FALSE, message=FALSE, gini_BC, fig.cap="The Lorenz curve and the Gini index values for BC claim",fig.height=6.15}
source("Gindex.R")
load(file="BC.Rdata")
Px <- rep(1,length(test$ClaimBC))
par(mfrow=c(2,4),mar=c(1,1,4,1),mgp=c(2.2,1,0),oma=c(0,0,0,0))
gingraph.poisgam_indep <- gini.index.graphic(glm.avgsev_indep.pred*glm.freq.pois_pred,
                                             test$ClaimBC,Px,Title="Indep Pois-Gamma")
gingraph.poisgam_dep <- gini.index.graphic(glm.avgsev.pois_pred*glm.freq.pois_pred,
                                           test$ClaimBC,Px,Title="Dep Pois-Gamma")
gingraph.zipgam_indep <- gini.index.graphic(glm.avgsev_indep.pred*glm.freq.zip_pred,
                                            test$ClaimBC,Px,Title="Indep ZIP-Gamma")
gingraph.zipgam_dep <- gini.index.graphic(glm.avgsev.zip_pred*glm.freq.zip_pred,
                                          test$ClaimBC,Px,Title="Dep ZIP-Gamma")
gingraph.2nnet_indep <- gini.index.graphic(nnet.avgsev_indep.pred*nnet.freq.pred,
                                           test$ClaimBC,Px,Title="Indep Two-parts NN")
gingraph.2nnet_dep <- gini.index.graphic(nnet.avgsev_dep.pred*nnet.freq.pred,
                                         test$ClaimBC,Px,Title="Dep Two-parts NN")
gingraph.Tweedglm <- gini.index.graphic(glm.tsev.pred,test$ClaimBC,Px,Title="Tweedie GLM")
gingraph.1nnet <- gini.index.graphic(nnet.tsev.pred,test$ClaimBC,Px,Title="One-part NN")

MSE_glm2ind <- sqrt(mean((glm.avgsev_indep.pred *glm.freq.pois_pred - test$ClaimBC)^2)) # indep two-part glm (pois + gamma)
MSE_glm2dep <- sqrt(mean((glm.avgsev.pois_pred  *glm.freq.pois_pred - test$ClaimBC)^2)) # dep two-part glm (pois + gamma)
MSE_glm2indz <- sqrt(mean((glm.avgsev_indep.pred *glm.freq.zip_pred - test$ClaimBC)^2)) # indep two-part glm (zip + gamma)
MSE_glm2depz <- sqrt(mean((glm.avgsev.zip_pred  *glm.freq.zip_pred - test$ClaimBC)^2)) # dep two-part glm (zip + gamma)
MSE_nnet2ind <- sqrt(mean((nnet.avgsev_indep.pred*nnet.freq.pred - test$ClaimBC)^2)) # indep two-part nnet
MSE_nnet2dep <- sqrt(mean((nnet.avgsev_dep.pred  *nnet.freq.pred - test$ClaimBC)^2)) # dep two-part nnet
MSE_nnet1 <- sqrt(mean((nnet.tsev.pred - test$ClaimBC)^2))    # one-part nnet
MSE_tweedie <- sqrt(mean((glm.tsev.pred - test$ClaimBC)^2))   # tweedie glm

MSEs <- as.data.frame(c(MSE_glm2ind,MSE_glm2dep,MSE_glm2indz,MSE_glm2depz,MSE_nnet2ind,MSE_nnet2dep,MSE_nnet1,MSE_tweedie))
colnames(MSEs) <- "BC"
rownames(MSEs) <- c("Indep Pois-Gamma","Dep Pois-Gamma","Indep ZIP-Gamma","Dep ZIP-Gamma","Indep-2P NN","Dep-2P NN","1P NN","Tweedie GLM")
```

# Gini Indices IM Claim

```{r, echo=FALSE, message=FALSE, gini_IM, fig.cap="The Lorenz curve and the Gini index values for IM claim",fig.height=6.15}
source("Gindex.R")
load(file="IM.Rdata")
Px <- rep(1,length(test$ClaimIM))
par(mfrow=c(2,4),mar=c(1,1,4,1),mgp=c(2.2,1,0),oma=c(0,0,0,0))
gingraph.poisgam_indep <- gini.index.graphic(glm.avgsev_indep.pred*glm.freq.pois_pred,
                                             test$ClaimIM,Px,Title="Indep Pois-Gamma")
gingraph.poisgam_dep <- gini.index.graphic(glm.avgsev.pois_pred*glm.freq.pois_pred,
                                           test$ClaimIM,Px,Title="Dep Pois-Gamma")
gingraph.zipgam_indep <- gini.index.graphic(glm.avgsev_indep.pred*glm.freq.zip_pred,
                                            test$ClaimIM,Px,Title="Indep ZIP-Gamma")
gingraph.zipgam_dep <- gini.index.graphic(glm.avgsev.zip_pred*glm.freq.zip_pred,
                                          test$ClaimIM,Px,Title="Dep ZIP-Gamma")
gingraph.2nnet_indep <- gini.index.graphic(nnet.avgsev_indep.pred*nnet.freq.pred,
                                           test$ClaimIM,Px,Title="Indep Two-parts NN")
gingraph.2nnet_dep <- gini.index.graphic(nnet.avgsev_dep.pred*nnet.freq.pred,
                                         test$ClaimIM,Px,Title="Dep Two-parts NN")
gingraph.Tweedglm <- gini.index.graphic(glm.tsev.pred,test$ClaimIM,Px,Title="Tweedie GLM")
gingraph.1nnet <- gini.index.graphic(nnet.tsev.pred,test$ClaimIM,Px,Title="One-part NN")

MSE_glm2ind <- sqrt(mean((glm.avgsev_indep.pred *glm.freq.pois_pred - test$ClaimIM)^2)) # indep two-part glm (pois + gamma)
MSE_glm2dep <- sqrt(mean((glm.avgsev.pois_pred  *glm.freq.pois_pred - test$ClaimIM)^2)) # dep two-part glm (pois + gamma)
MSE_glm2indz <- sqrt(mean((glm.avgsev_indep.pred *glm.freq.zip_pred - test$ClaimIM)^2)) # indep two-part glm (zip + gamma)
MSE_glm2depz <- sqrt(mean((glm.avgsev.zip_pred  *glm.freq.zip_pred - test$ClaimIM)^2)) # dep two-part glm (zip + gamma)
MSE_nnet2ind <- sqrt(mean((nnet.avgsev_indep.pred*nnet.freq.pred - test$ClaimIM)^2)) # indep two-part nnet
MSE_nnet2dep <- sqrt(mean((nnet.avgsev_dep.pred  *nnet.freq.pred - test$ClaimIM)^2)) # dep two-part nnet
MSE_nnet1 <- sqrt(mean((nnet.tsev.pred - test$ClaimIM)^2))    # one-part nnet
MSE_tweedie <- sqrt(mean((glm.tsev.pred - test$ClaimIM)^2))   # tweedie glm

MSEs$IM <- c(MSE_glm2ind,MSE_glm2dep,MSE_glm2indz,MSE_glm2depz,MSE_nnet2ind,MSE_nnet2dep,MSE_nnet1,MSE_tweedie)
```

# Gini Indices PN Claim

```{r, echo=FALSE, message=FALSE, gini_PN, fig.cap="The Lorenz curve and the Gini index values for PN claim",fig.height=6.15}
source("Gindex.R")
load(file="PN.Rdata")
Px <- rep(1,length(test$ClaimPN))
par(mfrow=c(2,4),mar=c(1,1,4,1),mgp=c(2.2,1,0),oma=c(0,0,0,0))
gingraph.poisgam_indep <- gini.index.graphic(glm.avgsev_indep.pred*glm.freq.pois_pred,
                                             test$ClaimPN,Px,Title="Indep Pois-Gamma")
gingraph.poisgam_dep <- gini.index.graphic(glm.avgsev.pois_pred*glm.freq.pois_pred,
                                           test$ClaimPN,Px,Title="Dep Pois-Gamma")
gingraph.zipgam_indep <- gini.index.graphic(glm.avgsev_indep.pred*glm.freq.zip_pred,
                                            test$ClaimPN,Px,Title="Indep ZIP-Gamma")
gingraph.zipgam_dep <- gini.index.graphic(glm.avgsev.zip_pred*glm.freq.zip_pred,
                                          test$ClaimPN,Px,Title="Dep ZIP-Gamma")
gingraph.2nnet_indep <- gini.index.graphic(nnet.avgsev_indep.pred*nnet.freq.pred,
                                           test$ClaimPN,Px,Title="Indep Two-parts NN")
gingraph.2nnet_dep <- gini.index.graphic(nnet.avgsev_dep.pred*nnet.freq.pred,
                                         test$ClaimPN,Px,Title="Dep Two-parts NN")
gingraph.Tweedglm <- gini.index.graphic(glm.tsev.pred,test$ClaimPN,Px,Title="Tweedie GLM")
gingraph.1nnet <- gini.index.graphic(nnet.tsev.pred,test$ClaimPN,Px,Title="One-part NN")

MSE_glm2ind <- sqrt(mean((glm.avgsev_indep.pred *glm.freq.pois_pred - test$ClaimPN)^2)) # indep two-part glm (pois + gamma)
MSE_glm2dep <- sqrt(mean((glm.avgsev.pois_pred  *glm.freq.pois_pred - test$ClaimPN)^2)) # dep two-part glm (pois + gamma)
MSE_glm2indz <- sqrt(mean((glm.avgsev_indep.pred *glm.freq.zip_pred - test$ClaimPN)^2)) # indep two-part glm (zip + gamma)
MSE_glm2depz <- sqrt(mean((glm.avgsev.zip_pred  *glm.freq.zip_pred - test$ClaimPN)^2)) # dep two-part glm (zip + gamma)
MSE_nnet2ind <- sqrt(mean((nnet.avgsev_indep.pred*nnet.freq.pred - test$ClaimPN)^2)) # indep two-part nnet
MSE_nnet2dep <- sqrt(mean((nnet.avgsev_dep.pred  *nnet.freq.pred - test$ClaimPN)^2)) # dep two-part nnet
MSE_nnet1 <- sqrt(mean((nnet.tsev.pred - test$ClaimPN)^2))    # one-part nnet
MSE_tweedie <- sqrt(mean((glm.tsev.pred - test$ClaimPN)^2))   # tweedie glm

MSEs$PN <- c(MSE_glm2ind,MSE_glm2dep,MSE_glm2indz,MSE_glm2depz,MSE_nnet2ind,MSE_nnet2dep,MSE_nnet1,MSE_tweedie)
```

# MSEs for all Type of Claim per Model
```{r}
MSEs
```

# Analysis of the Results

\begin{itemize}
  \item According to the MSE and Gini indices of given models, in BC claim one part neural network outperforms the other models, whereas two-part dependent GLM was the best for IM and PN claim.
  \item Note that the difference of performance between neural network and traditional GLM was greater when observed claim had a lot of outlier.
  \item Therefore, we may consider using neural network for predictive modeling of non-trivial dataset, whereas traditional GLM still works well with trivial dataset.
\end{itemize}

# Need of Compromise between two Objectives
![Interpretability and Performance](C:/Users/HimChan/Desktop/Daction/compromise.png)

# Future Works for this Project

\begin{itemize}
  \item Now we have two categories of benchmarks; one is GLM (for interpretability) and the other is neural network.
  \item Following work should be refining current GLM by incorporating longitudinal property or more sophisticated distrubutional assumption.
\end{itemize}